wandb: Appending key for api.wandb.ai to your netrc file: /home/dmura/.netrc
wandb: Currently logged in as: dmurairimukongya (team-azam). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/dmura/baseline-modal/wandb/run-20240318_113440-VOTFIzc9xnL6464nOBRh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run VOTFIzc9xnL6464nOBRh
wandb: ‚≠êÔ∏è View project at https://wandb.ai/team-azam/baseline-modal
wandb: üöÄ View run at https://wandb.ai/team-azam/baseline-modal/runs/VOTFIzc9xnL6464nOBRh
Namespace(annotation_path='ravdess_preprocessing/annotations.txt', arch='multimodalcnn', batch_size=2, begin_epoch=1, dampening=0.9, dataset='RAVDESS', device='cpu', fusion='ia', learning_rate=0.04, lr_patience=10, lr_steps=[40, 55, 65, 70, 200, 250], manual_seed=1, mask='softhard', model='multimodalcnn', momentum=0.9, n_classes=8, n_epochs=100, n_threads=16, no_train=True, no_val=True, num_heads=1, pretrain_path='EfficientFace_Trained_on_AffectNet7.pth.tar', result_path='./results', resume_path='', sample_duration=15, sample_size=224, store_name='RAVDESS_multimodalcnn_15', test=True, test_subset='test', video_norm_value=255, wandb_api_key='67854161c8597710d75225dc126932a363af1945', wandb_entity='team-azam', wandb_project='emotion-recognition-baseline', wandb_resume=False, wandb_run_id='S4JuaRNNGQ8pLEKgSEeK', wandb_tags='baseline', weight_decay=0.001)
Initializing efficientnet
[INFO] W&B run initialized successfully
validation at epoch 10000
validation at epoch 10000
/home/dmura/anaconda3/envs/idl-baseline-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [10000][1/104]	Time 9.49303 (9.49303)	Data 6.97053 (6.97053)	Loss 0.0913 (0.0913)	Prec@1 100.00000 (100.00000)	Prec@5 100.00000 (100.00000)
Epoch: [10000][2/104]	Time 0.78836 (5.14069)	Data 0.02464 (3.49759)	Loss 3.5535 (1.8224)	Prec@1 50.00000 (75.00000)	Prec@5 100.00000 (100.00000)
Epoch: [10000][3/104]	Time 0.69700 (3.65946)	Data 0.00462 (2.33326)	Loss 0.8141 (1.4863)	Prec@1 50.00000 (66.66666)	Prec@5 100.00000 (100.00000)
Epoch: [10000][4/104]	Time 0.62737 (2.90144)	Data 0.00444 (1.75106)	Loss 0.5582 (1.2543)	Prec@1 100.00000 (75.00000)	Prec@5 100.00000 (100.00000)
Epoch: [10000][5/104]	Time 0.64082 (2.44931)	Data 0.03983 (1.40881)	Loss 0.0001 (1.0034)	Prec@1 100.00000 (80.00000)	Prec@5 100.00000 (100.00000)
Epoch: [10000][6/104]	Time 0.62366 (2.14504)	Data 0.00644 (1.17508)	Loss 3.1817 (1.3665)	Prec@1 50.00000 (75.00000)	Prec@5 100.00000 (100.00000)
Epoch: [10000][7/104]	Time 0.70463 (1.93927)	Data 0.00349 (1.00771)	Loss 4.3243 (1.7890)	Prec@1 0.00000 (64.28571)	Prec@5 100.00000 (100.00000)
Epoch: [10000][8/104]	Time 0.58762 (1.77031)	Data 0.00577 (0.88247)	Loss 1.5999 (1.7654)	Prec@1 50.00000 (62.50000)	Prec@5 100.00000 (100.00000)
Epoch: [10000][9/104]	Time 0.58421 (1.63852)	Data 0.00497 (0.78497)	Loss 1.0031 (1.6807)	Prec@1 50.00000 (61.11111)	Prec@5 100.00000 (100.00000)
Epoch: [10000][10/104]	Time 0.64415 (1.53908)	Data 0.00428 (0.70690)	Loss 4.6971 (1.9823)	Prec@1 50.00000 (60.00000)	Prec@5 50.00000 (95.00000)
Epoch: [10000][11/104]	Time 0.64587 (1.45788)	Data 0.00842 (0.64340)	Loss 2.0299 (1.9867)	Prec@1 50.00000 (59.09091)	Prec@5 100.00000 (95.45454)
Epoch: [10000][12/104]	Time 0.58877 (1.38546)	Data 0.00394 (0.59011)	Loss 3.3279 (2.0984)	Prec@1 0.00000 (54.16667)	Prec@5 100.00000 (95.83334)
Epoch: [10000][13/104]	Time 0.60101 (1.32511)	Data 0.00376 (0.54501)	Loss 3.2576 (2.1876)	Prec@1 0.00000 (50.00000)	Prec@5 100.00000 (96.15385)
Epoch: [10000][14/104]	Time 0.62714 (1.27526)	Data 0.00424 (0.50638)	Loss 0.1676 (2.0433)	Prec@1 100.00000 (53.57143)	Prec@5 100.00000 (96.42857)
Epoch: [10000][15/104]	Time 0.58964 (1.22955)	Data 0.00352 (0.47286)	Loss 2.0145 (2.0414)	Prec@1 50.00000 (53.33333)	Prec@5 100.00000 (96.66666)
Epoch: [10000][16/104]	Time 0.58994 (1.18958)	Data 0.00396 (0.44355)	Loss 3.2120 (2.1146)	Prec@1 50.00000 (53.12500)	Prec@5 100.00000 (96.87500)
Epoch: [10000][17/104]	Time 0.71283 (1.16153)	Data 0.00417 (0.41771)	Loss 4.4170 (2.2500)	Prec@1 50.00000 (52.94118)	Prec@5 50.00000 (94.11765)
Epoch: [10000][18/104]	Time 0.62283 (1.13160)	Data 0.01044 (0.39508)	Loss 0.3011 (2.1417)	Prec@1 100.00000 (55.55556)	Prec@5 100.00000 (94.44444)
Epoch: [10000][19/104]	Time 0.60091 (1.10367)	Data 0.00453 (0.37453)	Loss 3.4258 (2.2093)	Prec@1 50.00000 (55.26316)	Prec@5 100.00000 (94.73684)
Epoch: [10000][20/104]	Time 0.65270 (1.08112)	Data 0.00529 (0.35606)	Loss 1.4001 (2.1688)	Prec@1 0.00000 (52.50000)	Prec@5 100.00000 (95.00000)
Epoch: [10000][21/104]	Time 0.83014 (1.06917)	Data 0.02000 (0.34006)	Loss 2.1350 (2.1672)	Prec@1 50.00000 (52.38095)	Prec@5 100.00000 (95.23810)
Epoch: [10000][22/104]	Time 0.51978 (1.04420)	Data 0.00512 (0.32484)	Loss 4.4977 (2.2732)	Prec@1 50.00000 (52.27273)	Prec@5 50.00000 (93.18182)
Epoch: [10000][23/104]	Time 0.59730 (1.02477)	Data 0.00649 (0.31100)	Loss 2.6995 (2.2917)	Prec@1 50.00000 (52.17391)	Prec@5 100.00000 (93.47826)
Epoch: [10000][24/104]	Time 0.61013 (1.00749)	Data 0.00571 (0.29828)	Loss 3.0548 (2.3235)	Prec@1 0.00000 (50.00000)	Prec@5 100.00000 (93.75000)
Epoch: [10000][25/104]	Time 0.57417 (0.99016)	Data 0.00651 (0.28660)	Loss 2.6287 (2.3357)	Prec@1 0.00000 (48.00000)	Prec@5 100.00000 (94.00000)
Epoch: [10000][26/104]	Time 0.55920 (0.97358)	Data 0.00609 (0.27582)	Loss 1.1254 (2.2892)	Prec@1 50.00000 (48.07692)	Prec@5 100.00000 (94.23077)
Epoch: [10000][27/104]	Time 0.63756 (0.96114)	Data 0.00842 (0.26591)	Loss 0.1891 (2.2114)	Prec@1 100.00000 (50.00000)	Prec@5 100.00000 (94.44444)
Epoch: [10000][28/104]	Time 0.55350 (0.94658)	Data 0.00376 (0.25655)	Loss 0.0952 (2.1358)	Prec@1 100.00000 (51.78571)	Prec@5 100.00000 (94.64286)
Epoch: [10000][29/104]	Time 0.60225 (0.93471)	Data 0.00508 (0.24788)	Loss 1.3507 (2.1087)	Prec@1 50.00000 (51.72414)	Prec@5 100.00000 (94.82758)
Epoch: [10000][30/104]	Time 0.59351 (0.92333)	Data 0.00505 (0.23978)	Loss 2.3069 (2.1153)	Prec@1 0.00000 (50.00000)	Prec@5 100.00000 (95.00000)
Epoch: [10000][31/104]	Time 0.55371 (0.91141)	Data 0.01243 (0.23245)	Loss 3.4965 (2.1599)	Prec@1 0.00000 (48.38710)	Prec@5 100.00000 (95.16129)
Epoch: [10000][32/104]	Time 0.61956 (0.90229)	Data 0.00397 (0.22531)	Loss 0.8960 (2.1204)	Prec@1 50.00000 (48.43750)	Prec@5 100.00000 (95.31250)
Epoch: [10000][33/104]	Time 0.62621 (0.89392)	Data 0.00422 (0.21861)	Loss 1.6476 (2.1061)	Prec@1 0.00000 (46.96970)	Prec@5 100.00000 (95.45454)
Epoch: [10000][34/104]	Time 0.59288 (0.88507)	Data 0.00566 (0.21235)	Loss 0.2453 (2.0513)	Prec@1 100.00000 (48.52941)	Prec@5 100.00000 (95.58823)
Epoch: [10000][35/104]	Time 0.60514 (0.87707)	Data 0.00658 (0.20647)	Loss 0.0003 (1.9927)	Prec@1 100.00000 (50.00000)	Prec@5 100.00000 (95.71429)
Epoch: [10000][36/104]	Time 0.59420 (0.86921)	Data 0.00664 (0.20092)	Loss 1.5771 (1.9812)	Prec@1 0.00000 (48.61111)	Prec@5 100.00000 (95.83334)
Epoch: [10000][37/104]	Time 0.74352 (0.86582)	Data 0.00756 (0.19569)	Loss 5.4568 (2.0751)	Prec@1 0.00000 (47.29730)	Prec@5 50.00000 (94.59460)
Epoch: [10000][38/104]	Time 0.53061 (0.85700)	Data 0.00651 (0.19071)	Loss 3.6818 (2.1174)	Prec@1 50.00000 (47.36842)	Prec@5 100.00000 (94.73684)
Epoch: [10000][39/104]	Time 0.59514 (0.85028)	Data 0.00695 (0.18600)	Loss 4.0668 (2.1674)	Prec@1 50.00000 (47.43590)	Prec@5 50.00000 (93.58974)
Epoch: [10000][40/104]	Time 0.59363 (0.84387)	Data 0.00880 (0.18157)	Loss 0.4302 (2.1240)	Prec@1 100.00000 (48.75000)	Prec@5 100.00000 (93.75000)
Epoch: [10000][41/104]	Time 0.60172 (0.83796)	Data 0.00657 (0.17730)	Loss 0.2020 (2.0771)	Prec@1 100.00000 (50.00000)	Prec@5 100.00000 (93.90244)
Epoch: [10000][42/104]	Time 0.58121 (0.83185)	Data 0.00686 (0.17324)	Loss 1.3722 (2.0603)	Prec@1 50.00000 (50.00000)	Prec@5 100.00000 (94.04762)
Epoch: [10000][43/104]	Time 0.61065 (0.82670)	Data 0.00605 (0.16936)	Loss 0.0190 (2.0128)	Prec@1 100.00000 (51.16279)	Prec@5 100.00000 (94.18604)
Epoch: [10000][44/104]	Time 0.60904 (0.82176)	Data 0.00564 (0.16564)	Loss 4.9183 (2.0789)	Prec@1 50.00000 (51.13636)	Prec@5 50.00000 (93.18182)
Epoch: [10000][45/104]	Time 0.61541 (0.81717)	Data 0.00535 (0.16207)	Loss 0.0528 (2.0338)	Prec@1 100.00000 (52.22222)	Prec@5 100.00000 (93.33334)
Epoch: [10000][46/104]	Time 0.53133 (0.81096)	Data 0.00567 (0.15867)	Loss 0.2571 (1.9952)	Prec@1 100.00000 (53.26087)	Prec@5 100.00000 (93.47826)
Epoch: [10000][47/104]	Time 0.58732 (0.80620)	Data 0.00537 (0.15541)	Loss 1.9986 (1.9953)	Prec@1 50.00000 (53.19149)	Prec@5 100.00000 (93.61702)
Epoch: [10000][48/104]	Time 0.58996 (0.80169)	Data 0.00569 (0.15229)	Loss 0.2581 (1.9591)	Prec@1 100.00000 (54.16667)	Prec@5 100.00000 (93.75000)
Epoch: [10000][49/104]	Time 0.48191 (0.79517)	Data 0.00576 (0.14930)	Loss 0.2226 (1.9237)	Prec@1 100.00000 (55.10204)	Prec@5 100.00000 (93.87755)
Epoch: [10000][50/104]	Time 0.61886 (0.79164)	Data 0.00537 (0.14642)	Loss 1.2938 (1.9111)	Prec@1 0.00000 (54.00000)	Prec@5 100.00000 (94.00000)
Epoch: [10000][51/104]	Time 0.59074 (0.78770)	Data 0.00528 (0.14366)	Loss 0.3776 (1.8810)	Prec@1 100.00000 (54.90196)	Prec@5 100.00000 (94.11765)
Epoch: [10000][52/104]	Time 0.54783 (0.78309)	Data 0.00664 (0.14102)	Loss 1.0138 (1.8643)	Prec@1 50.00000 (54.80769)	Prec@5 100.00000 (94.23077)
Epoch: [10000][53/104]	Time 0.61440 (0.77991)	Data 0.00668 (0.13849)	Loss 0.2619 (1.8341)	Prec@1 100.00000 (55.66038)	Prec@5 100.00000 (94.33962)
Epoch: [10000][54/104]	Time 0.66659 (0.77781)	Data 0.00734 (0.13606)	Loss 0.0865 (1.8017)	Prec@1 100.00000 (56.48148)	Prec@5 100.00000 (94.44444)
Epoch: [10000][55/104]	Time 0.57022 (0.77403)	Data 0.00715 (0.13371)	Loss 0.8540 (1.7845)	Prec@1 50.00000 (56.36364)	Prec@5 100.00000 (94.54546)
Epoch: [10000][56/104]	Time 0.63221 (0.77150)	Data 0.00750 (0.13146)	Loss 0.3520 (1.7589)	Prec@1 100.00000 (57.14286)	Prec@5 100.00000 (94.64286)
/home/dmura/anaconda3/envs/idl-baseline-env/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
Epoch: [10000][57/104]	Time 0.62228 (0.76888)	Data 0.00617 (0.12926)	Loss 1.0758 (1.7469)	Prec@1 50.00000 (57.01754)	Prec@5 100.00000 (94.73684)
Epoch: [10000][58/104]	Time 0.58611 (0.76573)	Data 0.00724 (0.12716)	Loss 0.0425 (1.7175)	Prec@1 100.00000 (57.75862)	Prec@5 100.00000 (94.82758)
Epoch: [10000][59/104]	Time 0.66202 (0.76397)	Data 0.00578 (0.12510)	Loss 2.1354 (1.7246)	Prec@1 50.00000 (57.62712)	Prec@5 100.00000 (94.91525)
Epoch: [10000][60/104]	Time 0.59132 (0.76110)	Data 0.00534 (0.12310)	Loss 0.7330 (1.7081)	Prec@1 50.00000 (57.50000)	Prec@5 100.00000 (95.00000)
Epoch: [10000][61/104]	Time 0.59066 (0.75830)	Data 0.00642 (0.12119)	Loss 0.3561 (1.6859)	Prec@1 100.00000 (58.19672)	Prec@5 100.00000 (95.08197)
Epoch: [10000][62/104]	Time 0.55902 (0.75509)	Data 0.00695 (0.11935)	Loss 0.5774 (1.6680)	Prec@1 50.00000 (58.06452)	Prec@5 100.00000 (95.16129)
Epoch: [10000][63/104]	Time 0.54126 (0.75169)	Data 0.00660 (0.11756)	Loss 0.0262 (1.6420)	Prec@1 100.00000 (58.73016)	Prec@5 100.00000 (95.23810)
Epoch: [10000][64/104]	Time 0.61108 (0.74950)	Data 0.00810 (0.11585)	Loss 0.3752 (1.6222)	Prec@1 100.00000 (59.37500)	Prec@5 100.00000 (95.31250)
Epoch: [10000][65/104]	Time 0.63163 (0.74768)	Data 0.01872 (0.11436)	Loss 1.8227 (1.6253)	Prec@1 50.00000 (59.23077)	Prec@5 100.00000 (95.38461)
Epoch: [10000][66/104]	Time 0.59200 (0.74532)	Data 0.00990 (0.11277)	Loss 0.6256 (1.6101)	Prec@1 100.00000 (59.84848)	Prec@5 100.00000 (95.45454)
Epoch: [10000][67/104]	Time 0.62717 (0.74356)	Data 0.00592 (0.11118)	Loss 0.5723 (1.5946)	Prec@1 50.00000 (59.70149)	Prec@5 100.00000 (95.52238)
Epoch: [10000][68/104]	Time 0.60804 (0.74157)	Data 0.00597 (0.10963)	Loss 0.5787 (1.5797)	Prec@1 50.00000 (59.55882)	Prec@5 100.00000 (95.58823)
Epoch: [10000][69/104]	Time 0.58988 (0.73937)	Data 0.00615 (0.10813)	Loss 0.2414 (1.5603)	Prec@1 100.00000 (60.14493)	Prec@5 100.00000 (95.65218)
Epoch: [10000][70/104]	Time 0.61674 (0.73762)	Data 0.00641 (0.10668)	Loss 0.9292 (1.5513)	Prec@1 50.00000 (60.00000)	Prec@5 100.00000 (95.71429)
Epoch: [10000][71/104]	Time 0.67332 (0.73671)	Data 0.01019 (0.10532)	Loss 2.9680 (1.5712)	Prec@1 0.00000 (59.15493)	Prec@5 100.00000 (95.77465)
Epoch: [10000][72/104]	Time 0.63009 (0.73523)	Data 0.00618 (0.10394)	Loss 1.6745 (1.5727)	Prec@1 0.00000 (58.33333)	Prec@5 100.00000 (95.83334)
Epoch: [10000][73/104]	Time 0.46271 (0.73150)	Data 0.00496 (0.10259)	Loss 3.3593 (1.5971)	Prec@1 0.00000 (57.53425)	Prec@5 100.00000 (95.89041)
Epoch: [10000][74/104]	Time 0.58420 (0.72951)	Data 0.00487 (0.10127)	Loss 0.3092 (1.5797)	Prec@1 100.00000 (58.10811)	Prec@5 100.00000 (95.94595)
Epoch: [10000][75/104]	Time 0.47516 (0.72612)	Data 0.00501 (0.09998)	Loss 0.3519 (1.5634)	Prec@1 100.00000 (58.66667)	Prec@5 100.00000 (96.00000)
Epoch: [10000][76/104]	Time 0.45850 (0.72260)	Data 0.00529 (0.09874)	Loss 0.6172 (1.5509)	Prec@1 50.00000 (58.55263)	Prec@5 100.00000 (96.05264)
Epoch: [10000][77/104]	Time 0.44587 (0.71900)	Data 0.00503 (0.09752)	Loss 0.7500 (1.5405)	Prec@1 100.00000 (59.09091)	Prec@5 100.00000 (96.10390)
Epoch: [10000][78/104]	Time 0.45463 (0.71561)	Data 0.00490 (0.09633)	Loss 0.5284 (1.5275)	Prec@1 100.00000 (59.61538)	Prec@5 100.00000 (96.15385)
Epoch: [10000][79/104]	Time 0.49386 (0.71281)	Data 0.00747 (0.09521)	Loss 0.0159 (1.5084)	Prec@1 100.00000 (60.12658)	Prec@5 100.00000 (96.20253)
Epoch: [10000][80/104]	Time 0.45270 (0.70955)	Data 0.00502 (0.09408)	Loss 1.6010 (1.5096)	Prec@1 0.00000 (59.37500)	Prec@5 100.00000 (96.25000)
Epoch: [10000][81/104]	Time 0.45254 (0.70638)	Data 0.00511 (0.09298)	Loss 0.7137 (1.4997)	Prec@1 100.00000 (59.87654)	Prec@5 100.00000 (96.29630)
Epoch: [10000][82/104]	Time 0.45637 (0.70333)	Data 0.00507 (0.09191)	Loss 2.2494 (1.5089)	Prec@1 0.00000 (59.14634)	Prec@5 100.00000 (96.34146)
Epoch: [10000][83/104]	Time 0.47016 (0.70052)	Data 0.00501 (0.09086)	Loss 0.5353 (1.4972)	Prec@1 100.00000 (59.63855)	Prec@5 100.00000 (96.38554)
Epoch: [10000][84/104]	Time 0.50006 (0.69814)	Data 0.00514 (0.08984)	Loss 1.0853 (1.4923)	Prec@1 50.00000 (59.52381)	Prec@5 100.00000 (96.42857)
Epoch: [10000][85/104]	Time 0.48853 (0.69567)	Data 0.00561 (0.08885)	Loss 0.2175 (1.4773)	Prec@1 100.00000 (60.00000)	Prec@5 100.00000 (96.47059)
Epoch: [10000][86/104]	Time 0.47178 (0.69307)	Data 0.00500 (0.08788)	Loss 0.2086 (1.4625)	Prec@1 100.00000 (60.46511)	Prec@5 100.00000 (96.51163)
Epoch: [10000][87/104]	Time 0.51867 (0.69106)	Data 0.00635 (0.08694)	Loss 0.2130 (1.4481)	Prec@1 100.00000 (60.91954)	Prec@5 100.00000 (96.55173)
Epoch: [10000][88/104]	Time 0.60062 (0.69003)	Data 0.01208 (0.08609)	Loss 0.2021 (1.4340)	Prec@1 100.00000 (61.36364)	Prec@5 100.00000 (96.59091)
Epoch: [10000][89/104]	Time 0.45745 (0.68742)	Data 0.00529 (0.08518)	Loss 0.7686 (1.4265)	Prec@1 100.00000 (61.79775)	Prec@5 100.00000 (96.62921)
Epoch: [10000][90/104]	Time 0.46032 (0.68490)	Data 0.00551 (0.08429)	Loss 0.7616 (1.4191)	Prec@1 50.00000 (61.66667)	Prec@5 100.00000 (96.66666)
Epoch: [10000][91/104]	Time 0.44996 (0.68232)	Data 0.00497 (0.08342)	Loss 0.8557 (1.4129)	Prec@1 50.00000 (61.53846)	Prec@5 100.00000 (96.70330)
Epoch: [10000][92/104]	Time 0.46167 (0.67992)	Data 0.00556 (0.08258)	Loss 0.8201 (1.4065)	Prec@1 50.00000 (61.41304)	Prec@5 100.00000 (96.73913)
Epoch: [10000][93/104]	Time 0.44835 (0.67743)	Data 0.00510 (0.08174)	Loss 2.9749 (1.4233)	Prec@1 0.00000 (60.75269)	Prec@5 100.00000 (96.77419)
Epoch: [10000][94/104]	Time 0.45634 (0.67508)	Data 0.00512 (0.08093)	Loss 2.2526 (1.4322)	Prec@1 50.00000 (60.63830)	Prec@5 100.00000 (96.80851)
Epoch: [10000][95/104]	Time 0.46319 (0.67285)	Data 0.00513 (0.08013)	Loss 0.3777 (1.4211)	Prec@1 100.00000 (61.05263)	Prec@5 100.00000 (96.84210)
Epoch: [10000][96/104]	Time 0.44380 (0.67046)	Data 0.00506 (0.07935)	Loss 0.6311 (1.4128)	Prec@1 100.00000 (61.45833)	Prec@5 100.00000 (96.87500)
Epoch: [10000][97/104]	Time 0.43134 (0.66799)	Data 0.00647 (0.07860)	Loss 0.1444 (1.3998)	Prec@1 100.00000 (61.85567)	Prec@5 100.00000 (96.90722)
Epoch: [10000][98/104]	Time 0.42715 (0.66554)	Data 0.00688 (0.07787)	Loss 0.8376 (1.3940)	Prec@1 50.00000 (61.73470)	Prec@5 100.00000 (96.93877)
Epoch: [10000][99/104]	Time 0.52998 (0.66417)	Data 0.00756 (0.07715)	Loss 0.1364 (1.3813)	Prec@1 100.00000 (62.12121)	Prec@5 100.00000 (96.96970)
Epoch: [10000][100/104]	Time 0.44103 (0.66194)	Data 0.00682 (0.07645)	Loss 0.0673 (1.3682)	Prec@1 100.00000 (62.50000)	Prec@5 100.00000 (97.00000)
Epoch: [10000][101/104]	Time 0.43273 (0.65967)	Data 0.00697 (0.07576)	Loss 0.2479 (1.3571)	Prec@1 100.00000 (62.87129)	Prec@5 100.00000 (97.02970)
Epoch: [10000][102/104]	Time 0.45192 (0.65763)	Data 0.00679 (0.07509)	Loss 0.7205 (1.3509)	Prec@1 50.00000 (62.74510)	Prec@5 100.00000 (97.05882)
Epoch: [10000][103/104]	Time 0.62227 (0.65729)	Data 0.00847 (0.07444)	Loss 0.9171 (1.3466)	Prec@1 50.00000 (62.62136)	Prec@5 100.00000 (97.08738)
Epoch: [10000][104/104]	Time 0.51499 (0.65592)	Data 0.02335 (0.07395)	Loss 0.1986 (1.3356)	Prec@1 100.00000 (62.98077)	Prec@5 100.00000 (97.11539)
wandb: - 0.002 MB of 0.002 MB uploaded
wandb: \ 0.002 MB of 0.002 MB uploaded
wandb: | 0.002 MB of 0.002 MB uploaded
wandb: / 0.002 MB of 0.002 MB uploaded
wandb: - 0.003 MB of 0.020 MB uploaded (0.001 MB deduped)
wandb: \ 0.003 MB of 0.022 MB uploaded (0.001 MB deduped)
wandb: | 0.022 MB of 0.022 MB uploaded (0.001 MB deduped)
wandb: üöÄ View run VOTFIzc9xnL6464nOBRh at: https://wandb.ai/team-azam/baseline-modal/runs/VOTFIzc9xnL6464nOBRh
wandb: Ô∏è‚ö° View job at https://wandb.ai/team-azam/baseline-modal/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0ODUyMTE0Mg==/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240318_113440-VOTFIzc9xnL6464nOBRh/logs
Traceback (most recent call last):
  File "main.py", line 190, in <module>
    test_loss, test_prec1 = val_epoch(10000, test_loader, model, criterion, opt,
ValueError: too many values to unpack (expected 2)
