{"annotation_path": "ravdess_preprocessing/annotations.txt", "wandb_api_key": "6f880b551b896cd396dbd54e2238ae37", "wandb_run_id": "U59L2mYIHF3yvtLBncid", "wandb_resume": false, "wandb_project": "emotion-recognition-baseline", "wandb_entity": "team-azam", "wandb_tags": "baseline", "result_path": "results", "store_name": "RAVDESS_multimodalcnn_15", "dataset": "RAVDESS", "n_classes": 8, "model": "multimodalcnn", "num_heads": 1, "device": "cpu", "sample_size": 224, "sample_duration": 15, "learning_rate": 0.04, "momentum": 0.9, "lr_steps": [40, 55, 65, 70, 200, 250], "dampening": 0.9, "weight_decay": 0.001, "lr_patience": 10, "batch_size": 8, "n_epochs": 100, "begin_epoch": 1, "resume_path": "", "pretrain_path": "EfficientFace_Trained_on_AffectNet7.pth.tar", "no_train": false, "no_val": false, "test": true, "test_subset": "test", "n_threads": 16, "video_norm_value": 255, "manual_seed": 1, "fusion": "ia", "mask": "softhard", "arch": "multimodalcnn"}